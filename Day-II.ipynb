{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d718b7bb",
   "metadata": {},
   "source": [
    "# Hands on session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2d5f3",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bc1c9",
   "metadata": {},
   "source": [
    "#### We will construct a linear model that explains the relationship a car's mileage (mpg) has with its other attributes. Here the mpg is a continuous variable and the dataset provided has the target variable in it which makes this a supervised learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227f406",
   "metadata": {},
   "source": [
    "## Import the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "\n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326b227",
   "metadata": {},
   "source": [
    "## Load the datset  \n",
    "Load the dataset using pd.read_csv() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData = pd.read_csv(\"auto-mpg.csv\")  \n",
    "cData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6df23",
   "metadata": {},
   "source": [
    "### Data Definition  \n",
    "\n",
    "8 variables: \n",
    "- MPG (miles per gallon), \n",
    "- cylinders, \n",
    "- engine displacement (cu. inches), \n",
    "- horsepower,\n",
    "- vehicle weight (lbs.), \n",
    "- time to accelerate from O to 60 mph (sec.),\n",
    "- model year (modulo 100), and \n",
    "- origin of car (1. American, 2. European,3. Japanese).\n",
    "- Also provided are the car labels (types) \n",
    "- Missing data values are marked by series of question marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9c458",
   "metadata": {},
   "source": [
    "## Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7e616",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963d994",
   "metadata": {},
   "source": [
    "### Print the shape and dimension of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dimension of the data is:')\n",
    "print(cData.shape)\n",
    "print('The size of the data:', cData.size)\n",
    "print('No of rows in the data:', cData.shape[0])\n",
    "print('No of columns in the data:', cData.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1d183",
   "metadata": {},
   "source": [
    "### Print the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1866b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cd0d8",
   "metadata": {},
   "source": [
    "### Get the number of unique observations in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284faeb",
   "metadata": {},
   "source": [
    "#### Since the car name has no significance on the miles of the car, we remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping/ignoring car_name \n",
    "cData = cData.drop('car name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ae163",
   "metadata": {},
   "source": [
    "#### Replacing numbers with countries in origin column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d803be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also replacing the categorical var with actual values\n",
    "cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})\n",
    "cData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6765f9",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac671b",
   "metadata": {},
   "source": [
    "Try to be as intuitive as possible and explore various combinations of plot. This is solely for better understanding and presentation of the data at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8d762",
   "metadata": {},
   "source": [
    "#### Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cData,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc096d",
   "metadata": {},
   "source": [
    "#### Count plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ddb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(cData['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(cData['cylinders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce06a2",
   "metadata": {},
   "source": [
    "#### Visualizing the distribution of  target variable 'mpg' using various plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200efb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(cData['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(cData['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(cData['mpg'],cData['weight'], hue=cData['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(cData['mpg'],cData['weight'], hue=cData['cylinders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01695993",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(cData['origin'],cData['mpg'], hue= cData['cylinders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87b85a",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9596956",
   "metadata": {},
   "source": [
    "#### Create Dummy Variables (One hot encoding) \n",
    "\n",
    "Values like 'america' cannot be read into an equation. Using substitutes like 1 for america, 2 for europe and 3 for asia would end up implying that european cars fall exactly half way between american and asian cars. We dont want to impose such an baseless assumption. So we perform one hot encoding on origin column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6175c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData = pd.get_dummies(cData, columns=['origin'])\n",
    "cData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5967007",
   "metadata": {},
   "source": [
    "#### Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35abe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1177262",
   "metadata": {},
   "source": [
    "A quick summary of the data columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449f1d6",
   "metadata": {},
   "source": [
    "#### Check if any values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01494e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad488871",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3cad2",
   "metadata": {},
   "source": [
    "In the above cells, we notice that horsepower is not recognized as a number but an object. So even if horsepower had any missing values, it will not be recognized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fb823",
   "metadata": {},
   "source": [
    "Let's get that sorted by converting horsepower to numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd93c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData = cData.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b31c08",
   "metadata": {},
   "source": [
    "Now we see that horsepower has six missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106cca1",
   "metadata": {},
   "source": [
    "#### Using median values to fill the missing values in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f120202",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianFiller = lambda x: x.fillna(x.median())\n",
    "cData = cData.apply(medianFiller,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc2fd",
   "metadata": {},
   "source": [
    "Now there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c796b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad26e4",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData2=cData[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'model year', 'origin_america', 'origin_asia',\n",
    "       'origin_europe']].apply(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047a826",
   "metadata": {},
   "source": [
    "#### Checking correlation between attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,7))\n",
    "plt.title('Correlation of Attributes', y=1.05, size=19)\n",
    "sns.heatmap(cData.corr(), cmap='plasma',annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391fe7e",
   "metadata": {},
   "source": [
    "#### From the above plot the following can be inferred\n",
    "- The correlation of 'acc' with all other attributes are less than 0.5\n",
    "- The correlation of 'cyl'(cylinder) with 'disp' (displacement) and 'wt' (weight) is high\n",
    "- The variable 'disp' is highly correlated to 'wt' (weight)\n",
    "- The variable 'hp' (horsepower) is highly correlated with 'cyl','wt' and 'disp' attributes\n",
    "#### Normally if we have many attributes or variables we would drop variables that have low correlation with other attributes, for example here, the acceleration variable has low correlation with other variables but since out data set is small, it is fine not to drop a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f02ec7",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ed47d",
   "metadata": {},
   "source": [
    "Let's do pair plot once again after one-hot encoding and removing missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d96b5",
   "metadata": {},
   "source": [
    "## Building model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb08cba",
   "metadata": {},
   "source": [
    "#### Seperate the target variable from the rest of the attributes/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independant variables\n",
    "X = cData2\n",
    "# the dependent variable\n",
    "y = cData[['mpg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c95d8",
   "metadata": {},
   "source": [
    "#### Using the function train_test_split we separate the dataset into train and test data. The number in the test_size mentions how much of the original dataset should be left for testing. \n",
    "Here X,y is split into x_train,x_test, y_train, y_test. 30% is the test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f20583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b24fc",
   "metadata": {},
   "source": [
    "### Create an instance of the linear regression.  \n",
    "Import Linear regression module from sklearn library. And then create an instance of linear regression funciton and save it to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression_model = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9df4a",
   "metadata": {},
   "source": [
    "### Use linear regression for training  \n",
    "The variable that has the linear regressor has an inbuilt function for training call fit(). Mention the train dataset inside the square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad83a0",
   "metadata": {},
   "source": [
    "### Now evaluate it. This gives us the R^2 score of our model.  \n",
    "Similarly we use another in built function called score to get the R^2 score of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b608de",
   "metadata": {},
   "source": [
    "### Now let's use our trained model on test data  \n",
    "Use the predict command on the test data to get the predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d082771",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=regression_model.predict(X_test)\n",
    "print(type(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9f0db",
   "metadata": {},
   "source": [
    "### Comparing the predicted reults with actual values  \n",
    "Printing the first ten values of the predicted results and the first 10 test data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175699fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    print(results[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e59c1c",
   "metadata": {},
   "source": [
    "### Evaluate the score for test data  \n",
    "Using the same score function get the R^2 score for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77744066",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d240d",
   "metadata": {},
   "source": [
    "#### As you can see the score is fairly high enough and the results are also fairly close to the actual label. Surprisingly the score for test data is better than train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f7ed9",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408e3c5",
   "metadata": {},
   "source": [
    "#### Let's use the K means clustering algorithm on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59489e",
   "metadata": {},
   "source": [
    "## Import the dataset and normalize it  \n",
    "We are using the same dataset so no need to load it again, just apply zscore normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData3=cData.apply(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f1ac4",
   "metadata": {},
   "source": [
    "## Find the number of K to initialize using Elbow method  \n",
    "For K means clustering, it is essential that we initialize the number of clusters we want. We are using Elbow method for that.\n",
    "In the code below out of 1 through 10 we need to find the optimal value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8063857",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=range(1,10)\n",
    "meanDistortions=[]\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(cData3)\n",
    "    prediction=model.predict(cData3)\n",
    "    meanDistortions.append(sum(np.min(cdist(cData3, model.cluster_centers_, 'euclidean'), axis=1)) / cData3.shape[0])\n",
    "\n",
    "\n",
    "plt.plot(clusters, meanDistortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average distortion')\n",
    "plt.title('Selecting k with the Elbow Method')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d17966",
   "metadata": {},
   "source": [
    "We could see a clear curve and cluster 4 seems to be the elbow point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf86aa",
   "metadata": {},
   "source": [
    "## Building Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a658e2f",
   "metadata": {},
   "source": [
    "### Create an instance of Kmeans  \n",
    "Mention the number of clusters with in it and fit it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=KMeans(4)\n",
    "final_model.fit(cData3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6e415",
   "metadata": {},
   "source": [
    "### Use the trained alogrithm to group the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e15e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=final_model.predict(cData3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042a8e2",
   "metadata": {},
   "source": [
    "### Add the predictions to the original dataset\n",
    "Load the original dataset, create a new column called group and put the predictions in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e551f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData4= pd.read_csv(\"auto-mpg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData4['group']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53aceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cData4.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574ebd",
   "metadata": {},
   "source": [
    "### Find the mean of each attribute in each group  \n",
    "First group the data by each group.  \n",
    "Use the df.mean() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clust = cData4.groupby(['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clust.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9378d",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeca883",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45560268",
   "metadata": {},
   "source": [
    "Read the data using pd.read_csv function and save it to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1dba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca62c0",
   "metadata": {},
   "source": [
    "## Splitting the labels from attributes/other columns  \n",
    "We split the labels from the rest of the attributes and apply normalization for the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class',axis=1)    \n",
    "Y = df['class']   \n",
    "X=X.apply(zscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9577a2f",
   "metadata": {},
   "source": [
    "## Splitting the dataset into train and test  \n",
    "The entire dataset is split using train_test_split function. 70% train and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3543f",
   "metadata": {},
   "source": [
    "### No of people who have diabetes and don't diabetes in Original, train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8507e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Dataset Diabetes     : {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 1]), (len(df.loc[df['class'] == 1])/len(df.index)) * 100))\n",
    "print(\"Original Dataset No Diabetes     : {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 0]), (len(df.loc[df['class'] == 0])/len(df.index)) * 100))\n",
    "print(\"\")\n",
    "print(\"Training Dataset Diabetes    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))\n",
    "print(\"Training Dataset No Diabetess   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))\n",
    "print(\"\")\n",
    "print(\"Test Dataset Diabetes        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))\n",
    "print(\"Test Dataset No Diabetes       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd41ab5",
   "metadata": {},
   "source": [
    "## Building model and evaluation\n",
    "Import Logistic regression from sklearn modul. Create an instance of the logistic regression, we also mention the type of solver we need, this is optinal. If no mentioned, the default value will be chosen. Once that is done, fit the model on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b4d5c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dcb78",
   "metadata": {},
   "source": [
    "### Use the model to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb21491",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59598ed9",
   "metadata": {},
   "source": [
    "### Compare the actual test data and the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(y_predict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b7728",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68577a71",
   "metadata": {},
   "source": [
    "#### Get R^2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = model.score(x_test, y_test)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1e0cc",
   "metadata": {},
   "source": [
    "#### Print the confusion matrix  \n",
    "The sklearn library has an module called metrics which has inbuilt function for printing the confusion matrix. From which we can get True positive, True negative, False positive and False negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cm=metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n",
    "                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c97f1",
   "metadata": {},
   "source": [
    "The confusion matrix\n",
    "\n",
    "True Positives (TP): we correctly predicted that they do have diabetes 48\n",
    "\n",
    "True Negatives (TN): we correctly predicted that they don't have diabetes 132\n",
    "\n",
    "False Positives (FP): we incorrectly predicted that they do have diabetes (a \"Type I error\") 14 Falsely predict positive Type I error\n",
    "\n",
    "False Negatives (FN): we incorrectly predicted that they don't have diabetes (a \"Type II error\") 37 Falsely predict negative Type II error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545d005",
   "metadata": {},
   "source": [
    "Sklearn has inbuilt functions for calculating various metircs like F1 score, Accuaracy, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a1c31",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba74662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.to_numpy() # the function requires numpy data, so we are converting dataframe to numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114d978",
   "metadata": {},
   "source": [
    "Using the accuracy score function we pass in the actual y values and predicted y values to get the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90dd3fa",
   "metadata": {},
   "source": [
    "Similarly we can calculate the other metrics using the respective in-built functions. We mention here average as 'binary' because we have only two classes in our label. If they were more than one you have to use 'macro' instead. There are also other options avaiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a3ca9",
   "metadata": {},
   "source": [
    "#### Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62560fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db668de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_predict, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21929331",
   "metadata": {},
   "source": [
    "#### Precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa16e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_predict, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad743a2c",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d777a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_predict, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b41147",
   "metadata": {},
   "source": [
    "# Support vector machines  \n",
    "# With and Without Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239566e1",
   "metadata": {},
   "source": [
    "## Load the data  \n",
    "Load the data into the notebook using pd.read_csv() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Part3 - vehicle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeace7f",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389b0ed",
   "metadata": {},
   "source": [
    "### Print the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb763fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of the data:', df.size)\n",
    "print('No of rows in the data:', df.shape[0])\n",
    "print('No of columns in the data:', df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f8573",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d84102",
   "metadata": {},
   "source": [
    "#### Splitting the data attribute columns from the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc635562",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('class',axis=1)\n",
    "y=df[['class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ac9ec",
   "metadata": {},
   "source": [
    "#### Imputing missing values  \n",
    "Converting non numerical values to numerical, if present. Then checking for missing values and filling them with median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8491356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47147dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianFiller = lambda x: x.fillna(x.median())\n",
    "x = x.apply(medianFiller,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0351c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78558e4b",
   "metadata": {},
   "source": [
    "#### Checking for zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19482e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y==0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318d804",
   "metadata": {},
   "source": [
    "Almost all the attributes are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569e209",
   "metadata": {},
   "source": [
    "### Correlation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,7))\n",
    "plt.title('Correlation of Attributes', y=1.05, size=19)\n",
    "sns.heatmap(x.corr(), cmap='plasma',annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c9772",
   "metadata": {},
   "source": [
    "#### From the correlation table the attributes from 'compactness' to 'scaled_radius_of_gyration.1' have strong correlation with other values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502baadd",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(x, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b4319",
   "metadata": {},
   "source": [
    "# SVM without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69056cab",
   "metadata": {},
   "source": [
    "## Splitting the dataset into train and test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdc123",
   "metadata": {},
   "source": [
    "Normalize the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb313797",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled=x.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6faa3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x_scaled,y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00dd1c7",
   "metadata": {},
   "source": [
    "## Defining a custom accuracy funtion  \n",
    "For every correct prediction a counter called correct increases by one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x]== predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8240800",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938813d0",
   "metadata": {},
   "source": [
    "### Initialize an instance of the svm function  \n",
    "Import SVM module from sklearn library and then use SVC ( which is the classifier function that uses support vector machines). Use the same hyper parameter as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab99e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.025, C=3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b37d5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12557d01",
   "metadata": {},
   "source": [
    "#### Since we are going to compare the perfomance of SVM algorithm with and without PCA let's track how long it takes to run both.  \n",
    "Use the datetime library to get the time at that particular moment. Measure the time taked to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3707f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t1=datetime.datetime.now()\n",
    "clf.fit(xtrain , ytrain)\n",
    "y_pred = clf.predict(xtest)\n",
    "t2=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed7841",
   "metadata": {},
   "source": [
    "#### Time taken  \n",
    "Print the time taked for the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time Taken:', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest2=ytest.to_numpy()\n",
    "np.resize(ytest2,(254,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1c702",
   "metadata": {},
   "source": [
    "### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy :', getAccuracy(ytest2 , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfb51a",
   "metadata": {},
   "source": [
    "# Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "covMatrix = np.cov(x_scaled,rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309501d1",
   "metadata": {},
   "source": [
    "## Create an instance of PCA \n",
    "Import PCA module from sklearn library. Inside the PCA function, mention the number of attribute columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "pca = PCA(n_components=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362c4d7",
   "metadata": {},
   "source": [
    "## Get the number of optimal eigen components required  \n",
    "First we fit the initialized PCA on the entire attribute column and then we plot a graph with eigen components in the x axis vs the variance exlpained along y axis. We need minimum number of eigen components that explain 90% of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7cc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(x_scaled)\n",
    "plt.bar(list(range(1,19)),pca.explained_variance_ratio_,alpha=0.5, align='center')\n",
    "plt.ylabel('Variation explained')\n",
    "plt.xlabel('eigen Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20465a54",
   "metadata": {},
   "source": [
    "### Cummulative plot of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(list(range(1,19)),np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Cum of variation explained')\n",
    "plt.xlabel('eigen Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a31ea2",
   "metadata": {},
   "source": [
    "#### From the graph we can understand that eigen value of 10 explains almost 100% of the variation. So we select only 10 components. Now we reduce 18 columns from the original dataset to 10 columns PCA data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aab31c",
   "metadata": {},
   "source": [
    "## Transform the input data using PCA  \n",
    "PCA will transform the input data of 18 columns into a data of 10 columns. It has mathematical functions that will do the job for you. All these 10 columns will have the same data contained in 18 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ce51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=10)\n",
    "pca3.fit(x_scaled)\n",
    "#print(pca3.components_)\n",
    "#print(pca3.explained_variance_ratio_)\n",
    "xpca = pca3.transform(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955325e",
   "metadata": {},
   "source": [
    "### Pairplotting the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(xpca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e117356",
   "metadata": {},
   "source": [
    "#### After applying the PCA we can understand that the variables are not related to one another and it is evident from the scatter plots and all those plots are normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc62b0",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e586df",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(xpca,y,test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09483669",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d802df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3=datetime.datetime.now()\n",
    "clf.fit(xtrain , ytrain)\n",
    "y_pred = clf.predict(xtest)\n",
    "t4=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7562f3",
   "metadata": {},
   "source": [
    "### Printing the time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time Taken :',t4-t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2715045",
   "metadata": {},
   "source": [
    "### Get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f411f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:',getAccuracy(ytest2 , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3bf6d",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0b31d",
   "metadata": {},
   "source": [
    "### On comparing the results we can clearly see that on using PCA we were able reduce the dimensions from 18 to 10 and accuracy isn't much affected but SVM when trained with PCA performed much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dbc9f",
   "metadata": {},
   "source": [
    "# Decision Tree as a regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8a3e1",
   "metadata": {},
   "source": [
    "## Import Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt   \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395bf35",
   "metadata": {},
   "source": [
    "## Load data  \n",
    "Load the dataset using pd.read_csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9dae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mpg_df = pd.read_csv('auto-mpg.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d924fc3",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c25a88",
   "metadata": {},
   "source": [
    "### Remove the insignificant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.drop('car name',axis=1,inplace= True)\n",
    "mpg_df.drop('model year',axis=1,inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15cdb8",
   "metadata": {},
   "source": [
    "### Change numbers to countries and do one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79068614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpg_df['origin'] = mpg_df['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})\n",
    "mpg_df = pd.get_dummies(mpg_df, columns=['origin'])\n",
    "mpg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cb6d8",
   "metadata": {},
   "source": [
    "### Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4626697",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df = mpg_df.convert_objects(convert_numeric=True)\n",
    "#cData.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianFiller = lambda x: x.fillna(x.median())\n",
    "mpg_df = mpg_df.apply(medianFiller,axis=0)\n",
    "mpg_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b20ad",
   "metadata": {},
   "source": [
    "### Split the labels from attributes and create training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg_df.copy(deep=True)\n",
    "X.drop('mpg',axis=1, inplace= True)\n",
    "y = mpg_df[['mpg']]  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134a03e",
   "metadata": {},
   "source": [
    "## Build model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2cb91",
   "metadata": {},
   "source": [
    "### Training\n",
    "Import decision tree regression (since we are using for regression problem) and fit it on training data. Use the same hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0, max_depth=3)\n",
    "\n",
    "regressor.fit(X_train , y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7b98b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6f495",
   "metadata": {},
   "source": [
    "### Evaluating using R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.to_numpy()\n",
    "y_test=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8414a5",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b33ed4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67d749",
   "metadata": {},
   "source": [
    "## Load Libraries and split it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2308c3",
   "metadata": {},
   "source": [
    "#### We are using breast cancer dataset which is readily available in sklearn datasets  \n",
    "We load it into the notebook using the command load_breast_cancer(). And then splitting the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer() \n",
    "X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state = 0) # compute minimum and maximum on the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c3160",
   "metadata": {},
   "source": [
    "## Create a pipeline\n",
    "In the below code, we have created a pipeline for SVM classification(SVC) and normalization using min_max scalar. We need import those functions before entring them in the pipeline. We also assign names to each function in apostrophe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC \n",
    "pipe = Pipeline([(\" scaler\", MinMaxScaler()), (\" svm\", SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd538be",
   "metadata": {},
   "source": [
    "## Fit the pipeline on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit( X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711290ee",
   "metadata": {},
   "source": [
    "## R^2 score on test set   \n",
    "Use the score function on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Test score: {:.2f}\". format( pipe.score( X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bf7b2",
   "metadata": {},
   "source": [
    "## Get the predicted labels  \n",
    "Use the predict function and fit it on X_test to get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bca19d",
   "metadata": {},
   "source": [
    "## Get the rest of metrics  \n",
    "sklearn's metric module has an single function called classification report that will print the necessary metrics. It takes in the test labels and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80148f35",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb7891",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n",
    "Again are using an dataset that is readily available in sklearn. We import the iris dataset using the command datasets.load_iris(). Then we split the labels from attributes and then split the dataset into train and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720043c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0RjYOukD8hA"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc61d12",
   "metadata": {},
   "source": [
    "## KNN  \n",
    "We will be using the KNN algorithm for classification. To perform hyperparameter tuning it is necessary that one must know about the parameters each algorithm has. In this example we will tune only two parameters of the KNN method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3d1b0",
   "metadata": {},
   "source": [
    "## Parameter Tuning  \n",
    "For parameter tuning, you need to create a dictionary with parameter names of the model as keys and their respective values in the value section. For example n_neighbors is the key and the respective value is any integers, hence we have provided a list of 1 to 9 in the value section. And similarly KNN has four different algorithms which are mentioned under the algorithm key  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe84b4",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XT-ufg0OD8hJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {'n_neighbors': list(range(1,9)),\n",
    "             'algorithm': ('auto', 'ball_tree', 'kd_tree' , 'brute') }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c21c07",
   "metadata": {},
   "source": [
    "### Create a KNN instance  \n",
    "Import KNN classifier from sklearn.neighbors and save it to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ac104",
   "metadata": {},
   "source": [
    "### Create a Gridsearch instance\n",
    "Import Gridsearch algorithm from sklearn and create an instance of it and save it to the variable. Mention the variable(which has the machine learning algorithm), then the parameter grid and third variable cross validation here is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edc7a1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0tFV4paD8hL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(knn_clf,param_grid,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63469bf9",
   "metadata": {},
   "source": [
    "### Fit on training  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f8573",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8s3tZueAD8hN",
    "outputId": "fe01eb8e-506b-4e0b-e631-57861c91a4da"
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431b01a",
   "metadata": {},
   "source": [
    "### Get the best parameters  \n",
    "Use the command best_params_ to get the best parameters from the option we provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a97ce",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opa0v3KsD8hP",
    "outputId": "c33b981a-0159-4dff-f835-56e37a770331"
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0415c",
   "metadata": {},
   "source": [
    "### Get the test score of all the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c502ab9",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f10rnJuPD8hR",
    "outputId": "4249ed93-0bf6-4c1e-d74f-499ed5a9dcb4"
   },
   "outputs": [],
   "source": [
    "gs.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b3909",
   "metadata": {},
   "source": [
    "### Print the scores for each combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24ca9c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyREPimND8hT",
    "outputId": "b4e159f1-655e-45e3-e7fe-838849165476"
   },
   "outputs": [],
   "source": [
    "gs.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529954f3",
   "metadata": {},
   "source": [
    "## Build model  using best parameters\n",
    "Here we import the KNN fron sklearn and save it to a variable. Inside the round brackets mention the parameters and their best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf2 = KNeighborsClassifier(algorithm='brute',n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83857234",
   "metadata": {},
   "source": [
    "### Train the algorithm, test it and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95205ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = knn_clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f80be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7034d",
   "metadata": {},
   "source": [
    "# Try it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ae979",
   "metadata": {},
   "source": [
    "You will be provided a dataset. The objective is to find whether a client is eligible for a loan or not based on the other columns(attributes). Since it is a binary classifier, use logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea5b2",
   "metadata": {},
   "source": [
    "Q1: Import the necessary libraries  \n",
    "You migh need: pandas, numpy, seaborn, matplotlib.pyplot, sklearn, scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f43bd6",
   "metadata": {},
   "source": [
    "Q2: Load the data into the notebook using pd.read_csv(file.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf0ee7",
   "metadata": {},
   "source": [
    "Q3: Remove the folowing columns  \n",
    "- Experience\n",
    "- Id\n",
    "- Zip code\n",
    "To drop multiple column use df.drop(['col1','col2'],inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a112c4",
   "metadata": {},
   "source": [
    "Q4: Take anyone dataset for now and perform EDA, try to implement the following function  \n",
    "- df.mean,\n",
    "- df.mode,\n",
    "- df.median,\n",
    "- df.describe, \n",
    "- df.quantile(q=0.25%), \n",
    "- df.corr()\n",
    "- df.cov()\n",
    "- df.info,\n",
    "- df.nunique,\n",
    "- df['col'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df6b4c",
   "metadata": {},
   "source": [
    "Q5: Plot various columns, try to use the following commands  \n",
    "- sns.distplot()\n",
    "- sns.pairplot()\n",
    "- sns.scatterplot()\n",
    "- sns.jointplot()\n",
    "- sns.countplot()\n",
    "- sns.boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a982e",
   "metadata": {},
   "source": [
    "Q6: Try to preprocess the data   \n",
    "- Convert class object to numerical object\n",
    "- Removing missing values or imputing with median values\n",
    "- One hot encoding if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae4700",
   "metadata": {},
   "source": [
    "Q7:Separate target variable(Personal Loan) from rest of the columns and split into training and test data  \n",
    "Use train_test_split() command  \n",
    "- X= attributes\n",
    "- Y= target column\n",
    "- Perform normalization on all columns of X before splitting into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae5ae9",
   "metadata": {},
   "source": [
    "Q8: Create an instance of logistic Regression and fit it on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893a9d8",
   "metadata": {},
   "source": [
    "Q9: Now use the trained model to predict by using predict() function on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa66b1",
   "metadata": {},
   "source": [
    "Q10: Get the confusion matrix, print accuracy, recall, f1_score and precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
